Tasks: The internship operated in a hybrid mode. The majority internship period we used to go to the university especially if we needed to conduct a meeting with the supervisors. For some days we used to work remotely. A typical working day started by discussing the tasks to be performed during the day at about 10:00am and it usually ended at around 5:00pm.  Since the majority of the internes did not have any previous knowledge about supply chains and creating graph visuals, we spent the first week researching into mentioned topics to understand the structure of the supply chains, to learn about the techniques used to model supply chains and finally to understand the benefits of modeling supply chain as a graph. During research, we learned about the components of supply chains and the different tiers (levels) it is composed of. In addition, we came across the SCOR model (supply chain operations reference) which has a purpose to aid in standardizing the process and create a measurable way to track results. Moreover, we found out about a type of graphs used to model supply chains called Petrinet. We were provided a dataset which holds randomly generated records to model a supply chain. After gaining adequate knowledge about supply chain, we began transforming the dataset into tables of nodes and edges which will be later used in visualization using python. During transformation process, we heavily relied on pandas and numpy libraries. The transformation pipeline is as follows:   Data Cleaning and Classifying Creating Nodes and edges Tables Reading Dataset transformation Tables   Firstly, we imported the data set using pandas, each table as a dataframe.  Secondly, we began the cleaning and transformation process. To start with, some columns were interpreted as strings though they were actually containing numeric or list items.  We created new columns from the original data with the correct data type. Moving on, some tables were split into smaller tables to differentiate between different types. Furthermore, since the dataset was randomly generated, some tables’ records did not make match with other tables’ records, we added new records to some tables to overcome this issue.  Additionally, we replaced all missing values with specific values we agreed on with the supervisor. Lastly, we removed all tables that will not be considered in later steps.  Thirdly, for each table, we searched for primary and foreign keys it includes. We assumed that the dataset did not contain any composite key. Subsequently, tables were classified into 3 classes according to the number of foreign keys in the table as follows: Class Number of Foreign keys Description Node 0 Main components of the supply chain Edge 2 Relations between nodes Property 1 Details about nodes/edges  *Tables that did not fall under any of these classes were handled individually.  Last but not least, all tables classified as nodes or edges were condensed into 2 tables; a node table and edges table. The edges table contains relations between nodes. Two approaches were considered while representing tables classified as property. The first approach is to add tables’ records as attributes to nodes or edges. Though this approach resulted in an accurate representation of the supply chain, it was difficult to visualize a property when a node or an edge contains a lot of properties. Another approach we considered is to create nodes for each property and to link it with an edge or a node. Nonetheless, in the majority of visualizing tools edges only connect between two nodes. Therefore, we decided to represent edges as nodes to be able to connect it with more than one node.   After creating edges and nodes tables, we selected couple visualization tools and each member of the team was assigned a specific tool one to test on our tables. The tools we used are Pyvis (python library), Networx (python library), Gephi and Neo4j.  I was responsible about creating a graph using Neo4j. Neo4j is a non-open source graph database management system developed by Neo4j and contains many tools for visualization such as Neo4j browser and Neo4j Bloom. Before using Neo4j, I needed to study cypher which is the query language used by Neo4j. Moreover, to integrate our python project with neo4j database, we needed to include Neo4j python diver to our project to connect with the graph database and to be able to execute cypher queries from python.  To visualize our tables, we needed to insert every record from nodes and edges tables into the database by executing specific cypher queries.  Figures in the Appendix section shows the visualization created using both mentioned approaches and obtained using different tools. Figure 1 shows the result of visualizing properties as attributes using Pyvis. It is obvious that the attributes are not easily readable and sometimes it can overflow. Figure 2, 3,4 show a zoomed out picture of the whole data set through representing nodes as edges and properties using Pyvis, Neo4j Bloom and Neo4j Browser respectively. Figure 5 shows a zoomed in picture of the supply chain. Each class was assigned a different color to facilitate the process of differentiating between classes easily.   Internship 