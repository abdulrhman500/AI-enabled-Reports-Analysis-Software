Activities:In this internship. I was trying to research and apply different techniques of combining differenttrained models to improve their performance metric. I did my research on two tasks: the first wasSpeech Recognition and the second was Neural Machine Translation. This task was an individualtask however, I was supervised by two mentors: Andrew Baher for Speech Recognition andEvram Narrouz for Neural Machine Translation. I was working five days per week for 8 hoursevery day. I could either work from home or go work at the company and I chose to go to thecompany once or twice a week. My typical working day started with having a meeting witheither one of my supervisors that I was supposed to work on his task to plan the day and what Iam supposed to do for the day. Most of the time, I was performing experiments by trainingmodels with different hyperparameters. While waiting for the models to finish training, I used toread articles on new ways to average these models or new techniques to come up with newexperiments to perform. After training the models, I ran the average models code that I haveprepared for the experiment and when the results are ready, I communicate them to mysupervisor in order to discuss their gains and whether they were significant or not. Then, wediscuss the implications of our findings and propose new ideas to experiment. The tools that Ihave used are different between the two tasks however, Python was the main language that I usedto code with, azure storage was the main database that stored all the models and datasets, andamlt was the main platform to train the different models on it. The timeline of the internshipwent as follows: I first started by reading different papers about model averaging and discussedthem with my supervisor until we decided that we should start working with one of them. Inorder to execute that method, we had to train 16 different models so I was asked to write theconfiguration files for training speech recognition models with different hyperparameters andsubmitting them to training on Azure itp cluster machines. While waiting for them to finishtraining I was supposed to design the algorithm for averaging them. I was supposed to do that ona program called AEther for multiprocessing, but there was a problem that AEther could not givefeedback in the same module for each step so I had to write the code from scratch and that’s whatI did. I spent almost 2-3 weeks writing the code for averaging and integrating it with the codebase of Microsoft for evaluating their speech recognition models. After getting the first resultsfrom the speech recognition experiment, we decided that I should do the same with the NMTtask and embed my averaging code with TorchSpeech modules and this took me another twoweeks. This took me a total of six weeks to run one experiment on each task and I had 2 weeksleft for my internship. That’s when my supervisors asked  me to extend my internship to continuemy experiments on the two tasks. For the last 6 weeks, I started exploring with my supervisorsdifferent ideas for each task, some of them included exploring larger datasets to see how muchgain the algorithm would have. I also explored the custom translators in NMT, where the modeltrained on a domain specific dataset like Medicine did well on this dataset but it usually did wayworse on the basic English sentences so I was asked to try model averaging on this model tobalance between its performance on the domain specific dataset and the general one as well.At the end of my internship, I managed to deliver the average models that usually did a little bitbetter than the stand alone trained models. This applied on both tasks and on the differentdatasets and domain specific tasks.Internship 