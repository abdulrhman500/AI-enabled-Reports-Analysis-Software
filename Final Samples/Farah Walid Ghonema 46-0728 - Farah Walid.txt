Tasks: I joined the web development sector at VERO to work on a face recognition application project for IHOP restaurants, an American multinational pancake house restaurant chain that specializes in American breakfast foods. I worked as a full stack developer and joined another colleague to work on this project. Together we managed to fully develop and deliver the application successfully. The application is launched on a touch screen at the IHOP restaurant in Cairo Festival City Mall. Its main idea is to ask people to smile, detect their faces and smiles, and based on these detections it makes an assumption about their mood and suggests a dish from the IHOP menu to match the client’s mood. It also has the IHOP menu available, if someone only wants to see the menu. The project was written in JavaScript, HTML and CSS, and we used the ElectronJS Framework. I was not familiar at first with the Framework, so I had to do some research to introduce myself to the framework. I also had to take several courses in order to learn how to write CSS and proper HTML files since I did not work with them professionally before. Next, me and my partner both researched the face recognition libraries available and chose to use the MediaPipe library provided by Google to recognize faces in our application. My daily routine involved joining the online scrum meetings with the rest of the company, having a separate meeting with my colleague to discuss and assign our tasks, and then I sit on my desk at home and begin by first pulling all the updates pushed on GitHub and then start coding my assigned tasks. At the end of the day, I would commit and push my work on GitHub and so will my co-worker. I visited the office once a week to work there with my colleague and we had several meetings with the IHOP board. My tasks involved initializing the desktop application, writing HTML and CSS files to build and style the pages of the application and building the algorithm to detect people’s mood. The algorithm mainly was based on detecting faces using MediaPipe and using the MediaPipe landmarks (coordinates) to calculate how big the person is smiling (the bigger the smile the happier a user is). A score is calculated for each user’s smile using our developed score function, and then the user’s mood is categorized either as happy or sad. Following that, a dish is suggested from the menu corresponding to the correct mood category. We faced many challenges while coding. One of the challenges faced was that the MediaPipe library took so long to load, and until it loaded the camera canvas was frozen on the screen. In order to overcome this challenge, I decided to divide the application into threads, so that we can load the MediaPipe library at the beginning of the application’s launch, so the screen does not freeze. Internship 