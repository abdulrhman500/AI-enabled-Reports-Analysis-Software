{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original class distribution\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def plot_class_distribution(list_,labels,message = \"Original Class Distribution\"):\n",
    "    unique, counts = np.unique(list_, return_counts=True)\n",
    "    colors =['green','yellow']\n",
    "    if len(unique) == 3 :\n",
    "        colors.append('red')\n",
    "        \n",
    "    plt.bar(unique, counts, color=colors)\n",
    "    plt.title(message)\n",
    "    \n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_class_distribution_pie(list_, labels, message=\"Original Class Distribution\"):\n",
    "    unique, counts = np.unique(list_, return_counts=True)\n",
    "    plt.pie(counts, labels=labels, colors=['green','yellow', 'red'], autopct='%1.1f%%')\n",
    "    plt.title(message)\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def read_csv_data(csv_file):\n",
    "    data = pd.read_csv(csv_file)\n",
    "    names = data['name'].tolist()\n",
    "    labels = data['label'].tolist()\n",
    "    texts = data['text'].tolist()\n",
    "    return names, labels, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_uinque_labels(labels):\n",
    "    unique = [0,0,0]\n",
    "    for label in labels:\n",
    "        if label.lower() =='approved':\n",
    "            unique[0] += 1\n",
    "        elif label.lower() == 'reject':\n",
    "            unique[1] += 1\n",
    "        else:\n",
    "            unique[2] += 1        \n",
    "    return {'approved':unique[0],'reject':unique[1],'pending':unique[2]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'approved': 198, 'reject': 8, 'pending': 30}\n"
     ]
    }
   ],
   "source": [
    "csv_file = 'Dataset.csv'\n",
    "names, labels, texts = read_csv_data(csv_file)\n",
    "\n",
    "print(count_uinque_labels(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data import Data\n",
    "\n",
    "def addToData1(names, labels, texts, data_obj, vectorization_technique):\n",
    "    for name, label, text in zip(names, labels, texts):\n",
    "        vector = vectorization_technique(text.strip()).numpy().tolist()\n",
    "        data_obj.add_data(name.strip(), vector, label.strip())\n",
    "    return data_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Data.Data at 0x22ae9dcc050>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from USE import apply_USE \n",
    "obj = Data()\n",
    "addToData1(names, labels, texts, obj, apply_USE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number:  236\n",
      "{'approved': 198, 'reject': 8, 'pending': 30}\n"
     ]
    }
   ],
   "source": [
    "X = obj.get_column(obj.col_document_vector)\n",
    "y = obj.get_column(obj.col_decision)\n",
    "print(\"Total number: \",len(X))\n",
    "# set pending to reject\n",
    "  \n",
    "labels_count = count_uinque_labels(y)        \n",
    "print(labels_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "approvals = labels_count['approved']\n",
    "rejects = labels_count['reject']\n",
    "pending = labels_count['pending']\n",
    "# class_proportions = 'auto'\n",
    "upsample_class_proportions = {'Approved':approvals, 'Reject':rejects*2,'Pending':pending}\n",
    "# upsample_class_proportions='auto'\n",
    "smote = SMOTE(random_state=42, sampling_strategy=upsample_class_proportions)\n",
    "\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Resampling:  Counter({'Approved': 198, 'Pending': 30, 'Reject': 8})\n",
      "After Resampling:  Counter({'Approved': 198, 'Pending': 30, 'Reject': 16})\n"
     ]
    }
   ],
   "source": [
    "# original_labels = y\n",
    "# resampled_labels = y_resampled\n",
    "# plot_class_distribution_pie(original_labels, labels=['approved', 'pending','rejected'], message=\"Original Class Distribution\")\n",
    "# plot_class_distribution_pie(resampled_labels, labels=['approved', 'pending','rejected'], message=\"Resampled Class Distribution\")\n",
    "\n",
    "# plot_class_distribution(original_labels, labels=['Class 0', 'Class 1'], message=\"Original Class Distribution\")\n",
    "# plot_class_distribution(resampled_labels, labels=['Class 0', 'Class 1'], message=\"Resampled Class Distribution\")\n",
    "print(\"Before Resampling: \",Counter(y))\n",
    "print(\"After Resampling: \",Counter(y_resampled))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Approved       0.85      1.00      0.92        41\n",
      "     Pending       1.00      0.14      0.25         7\n",
      "      Reject       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.86        49\n",
      "   macro avg       0.62      0.38      0.39        49\n",
      "weighted avg       0.86      0.86      0.81        49\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Abdulrhman Fahmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Abdulrhman Fahmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Abdulrhman Fahmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## auto \n",
    "```\n",
    "Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "    Approved       0.94      0.78      0.85        40\n",
    "     Pending       0.80      0.92      0.86        39\n",
    "      Reject       0.98      1.00      0.99        40\n",
    "\n",
    "    accuracy                           0.90       119\n",
    "   macro avg       0.91      0.90      0.90       119\n",
    "weighted avg       0.91      0.90      0.90       119\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## approve *1 , reject *2 , pending *2:\n",
    "```\n",
    "Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "    Approved       0.80      0.97      0.88        36\n",
    "     Pending       0.91      0.62      0.74        16\n",
    "      Reject       0.00      0.00      0.00         3\n",
    "\n",
    "    accuracy                           0.82        55\n",
    "   macro avg       0.57      0.53      0.54        55\n",
    "weighted avg       0.79      0.82      0.79        55\n",
    "\n",
    "```\n",
    "## approve *1 , reject *2 , pending:\n",
    "```\n",
    "Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "    Approved       0.85      1.00      0.92        41\n",
    "     Pending       1.00      0.14      0.25         7\n",
    "      Reject       0.00      0.00      0.00         1\n",
    "\n",
    "    accuracy                           0.86        49\n",
    "   macro avg       0.62      0.38      0.39        49\n",
    "weighted avg       0.86      0.86      0.81        49\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
