{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original class distribution\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def plot_class_distribution(list_,labels,message = \"Original Class Distribution\"):\n",
    "    unique, counts = np.unique(list_, return_counts=True)\n",
    "    colors =['green','yellow']\n",
    "    if len(unique) == 3 :\n",
    "        colors.append('red')\n",
    "        \n",
    "    plt.bar(unique, counts, color=colors)\n",
    "    \n",
    "    plt.title(message)\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_class_distribution_pie(list_, labels, message=\"Original Class Distribution\"):\n",
    "    unique, counts = np.unique(list_, return_counts=True)\n",
    "    plt.pie(counts, labels=labels, colors=['green','yellow', 'red'], autopct='%1.1f%%')\n",
    "    plt.title(message)\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def read_csv_data(csv_file):\n",
    "    data = pd.read_csv(csv_file)\n",
    "    names = data['name'].tolist()\n",
    "    labels = data['label'].tolist()\n",
    "    texts = data['text'].tolist()\n",
    "    return names, labels, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_uinque_labels(labels):\n",
    "    unique = [0,0,0]\n",
    "    for label in labels:\n",
    "        if label.lower() =='approved':\n",
    "            unique[0] += 1\n",
    "        elif label.lower() == 'reject':\n",
    "            unique[1] += 1\n",
    "        else:\n",
    "            unique[2] += 1        \n",
    "    return {'approved':unique[0],'reject':unique[1],'pending':unique[2]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'approved': 198, 'reject': 8, 'pending': 30}\n"
     ]
    }
   ],
   "source": [
    "csv_file = 'Dataset.csv'\n",
    "names, labels, texts = read_csv_data(csv_file)\n",
    "\n",
    "print(count_uinque_labels(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data import Data\n",
    "\n",
    "def addToData1(names, labels, texts, data_obj, vectorization_technique):\n",
    "    for name, label, text in zip(names, labels, texts):\n",
    "        vector = vectorization_technique(text.strip()).numpy().tolist()\n",
    "        data_obj.add_data(name.strip(), vector, label.strip())\n",
    "    return data_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from USE import apply_USE \n",
    "obj = Data()\n",
    "addToData1(names, labels, texts, obj, apply_USE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number:  236\n",
      "{'approved': 198, 'reject': 8, 'pending': 30}\n"
     ]
    }
   ],
   "source": [
    "X = obj.get_column(obj.col_document_vector)\n",
    "y = obj.get_column(obj.col_decision)\n",
    "print(\"Total number: \",len(X))\n",
    "# set pending to reject\n",
    "# for i in range(len(y)):\n",
    "#     if y[i] == 'Pending':\n",
    "#         y[i]='Reject'  \n",
    "labels_count = count_uinque_labels(y)        \n",
    "print(labels_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "approvals = labels_count['approved']\n",
    "rejects = labels_count['reject']\n",
    "pending = labels_count['pending']\n",
    "\n",
    "\n",
    "down_ration =1\n",
    "downsample_class_proportions = {'Approved':int(approvals*down_ratio), 'Reject':rejects,'Pending': pending}\n",
    "# downsample_class_proportions='auto'\n",
    "rus = RandomUnderSampler(random_state=42,sampling_strategy=downsample_class_proportions)\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Resampling:  Counter({'Approved': 198, 'Pending': 30, 'Reject': 8})\n",
      "After Resampling:  Counter({'Approved': 198, 'Pending': 30, 'Reject': 8})\n"
     ]
    }
   ],
   "source": [
    "# original_labels = y\n",
    "# resampled_labels = y_resampled\n",
    "# plot_class_distribution_pie(original_labels, labels=['approved', 'pending','rejected'], message=\"Original Class Distribution\")\n",
    "# plot_class_distribution_pie(resampled_labels, labels=['approved', 'pending','rejected'], message=\"Resampled Class Distribution\")\n",
    "\n",
    "# plot_class_distribution(original_labels, labels=['Class 0', 'Class 1'], message=\"Original Class Distribution\")\n",
    "# plot_class_distribution(resampled_labels, labels=['Class 0', 'Class 1'], message=\"Resampled Class Distribution\")\n",
    "print(\"Before Resampling: \",Counter(y))\n",
    "print(\"After Resampling: \",Counter(y_resampled))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Approved       0.81      1.00      0.90        39\n",
      "     Pending       0.00      0.00      0.00         7\n",
      "      Reject       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.81        48\n",
      "   macro avg       0.27      0.33      0.30        48\n",
      "weighted avg       0.66      0.81      0.73        48\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Abdulrhman Fahmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Abdulrhman Fahmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Abdulrhman Fahmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
