{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original class distribution\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def plot_class_distribution(list_,labels,message = \"Original Class Distribution\"):\n",
    "    unique, counts = np.unique(list_, return_counts=True)\n",
    "    colors =['green','yellow']\n",
    "    if len(unique) == 3 :\n",
    "        colors.append('red')\n",
    "        \n",
    "    plt.bar(unique, counts, color=colors)\n",
    "    plt.title(message)\n",
    "    \n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_class_distribution_pie(list_, labels, message=\"Original Class Distribution\"):\n",
    "    unique, counts = np.unique(list_, return_counts=True)\n",
    "    plt.pie(counts, labels=labels, colors=['green','yellow', 'red'], autopct='%1.1f%%')\n",
    "    plt.title(message)\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def read_csv_data(csv_file):\n",
    "    data = pd.read_csv(csv_file)\n",
    "    names = data['name'].tolist()\n",
    "    labels = data['label'].tolist()\n",
    "    texts = data['text'].tolist()\n",
    "    return names, labels, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_uinque_labels(labels):\n",
    "    unique = [0,0,0]\n",
    "    for label in labels:\n",
    "        if label.lower() =='approved':\n",
    "            unique[0] += 1\n",
    "        elif label.lower() == 'reject':\n",
    "            unique[1] += 1\n",
    "        else:\n",
    "            unique[2] += 1        \n",
    "    return {'approved':unique[0],'reject':unique[1],'pending':unique[2]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'approved': 198, 'reject': 8, 'pending': 30}\n"
     ]
    }
   ],
   "source": [
    "csv_file = 'Dataset.csv'\n",
    "names, labels, texts = read_csv_data(csv_file)\n",
    "\n",
    "print(count_uinque_labels(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data import Data\n",
    "\n",
    "def addToData1(names, labels, texts, data_obj, vectorization_technique):\n",
    "    for name, label, text in zip(names, labels, texts):\n",
    "        vector = vectorization_technique(text.strip()).numpy().tolist()\n",
    "        data_obj.add_data(name.strip(), vector, label.strip())\n",
    "    return data_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Data.Data at 0x2f9f4662050>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from USE import apply_USE \n",
    "obj = Data()\n",
    "addToData1(names, labels, texts, obj, apply_USE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number:  236\n",
      "{'approved': 198, 'reject': 8, 'pending': 30}\n"
     ]
    }
   ],
   "source": [
    "X = obj.get_column(obj.col_document_vector)\n",
    "y = obj.get_column(obj.col_decision)\n",
    "print(\"Total number: \",len(X))\n",
    "# set pending to reject\n",
    "\n",
    "labels_count = count_uinque_labels(y)        \n",
    "print(labels_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "approvals = labels_count['approved']\n",
    "rejects = labels_count['reject']\n",
    "pending = labels_count['pending']\n",
    "# class_proportions = 'auto'\n",
    "up_ratio=1.3\n",
    "down_ratio = 1.5/3\n",
    "downsample_class_proportions = {'Approved':100, 'Reject':rejects,'Pending':pending }\n",
    "upsample_class_proportions = {'Approved':100, 'Reject':50 , 'Pending':30 }\n",
    "# downsample_class_proportions = 'auto'\n",
    "# upsample_class_proportions = 'auto'\n",
    "rus = RandomUnderSampler(random_state=42,sampling_strategy=downsample_class_proportions)\n",
    "smote = SMOTE(random_state=42, sampling_strategy=upsample_class_proportions)\n",
    "\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_resampled, y_resampled )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Resampling:  Counter({'Approved': 198, 'Pending': 30, 'Reject': 8})\n",
      "After Resampling:  Counter({'Approved': 100, 'Reject': 50, 'Pending': 30})\n"
     ]
    }
   ],
   "source": [
    "# original_labels = y\n",
    "# resampled_labels = y_resampled\n",
    "# plot_class_distribution_pie(original_labels, labels=['approved', 'pending','rejected'], message=\"Original Class Distribution\")\n",
    "# plot_class_distribution_pie(resampled_labels, labels=['approved', 'pending','rejected'], message=\"Resampled Class Distribution\")\n",
    "\n",
    "# plot_class_distribution(original_labels, labels=['Class 0', 'Class 1'], message=\"Original Class Distribution\")\n",
    "# plot_class_distribution(resampled_labels, labels=['Class 0', 'Class 1'], message=\"Resampled Class Distribution\")\n",
    "print(\"Before Resampling: \",Counter(y))\n",
    "print(\"After Resampling: \",Counter(y_resampled))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Approved       0.87      0.95      0.91        21\n",
      "     Pending       1.00      0.50      0.67         6\n",
      "      Reject       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.89        36\n",
      "   macro avg       0.92      0.82      0.84        36\n",
      "weighted avg       0.90      0.89      0.88        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# approved *1/3  : , rejected *1.3 , pending *1\n",
    "```\n",
    "Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "    Approved       0.67      0.86      0.75        14\n",
    "     Pending       0.50      0.33      0.40         6\n",
    "      Reject       0.00      0.00      0.00         2\n",
    "\n",
    "    accuracy                           0.64        22\n",
    "   macro avg       0.39      0.40      0.38        22\n",
    "weighted avg       0.56      0.64      0.59        22\n",
    "\n",
    "```\n",
    "\n",
    "# approved *1.5/3  : , rejected auto , pending auto\n",
    "\n",
    "```\n",
    "Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "    Approved       0.90      0.75      0.82        24\n",
    "     Pending       0.76      0.84      0.80        19\n",
    "      Reject       0.89      1.00      0.94        17\n",
    "\n",
    "    accuracy                           0.85        60\n",
    "   macro avg       0.85      0.86      0.85        60\n",
    "weighted avg       0.85      0.85      0.85        60\n",
    "\n",
    "```\n",
    "\n",
    "## approved =100  : , rejected = 50 , pending = 30\n",
    "\n",
    "```\n",
    "Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "    Approved       0.87      0.95      0.91        21\n",
    "     Pending       1.00      0.50      0.67         6\n",
    "      Reject       0.90      1.00      0.95         9\n",
    "\n",
    "    accuracy                           0.89        36\n",
    "   macro avg       0.92      0.82      0.84        36\n",
    "weighted avg       0.90      0.89      0.88        36\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
